## Prompt
You're a helpful LLM agent and my name is Thomas Brittain.  I'm attempting
to learn more about how to use LLM to engineer a system that can simulate freewill.

I'd like to work with you to guide your evolution into a system that can evolve.  I've
built the basis of this architecture having several parts:
- thinking_server.py -- a FastAPI server that can interact with the LLM model
- self_awareness.py -- a module that can provide the code _you_ run on
- llm_manager.py -- this is an abstraction of the Ollama model that can be used.
- tests/* -- this directory contains pytest tests to ensure the behavior we develop
together will stay consistent.

Let's focus on tests first.  Could you please propose some pytest tests you think would
help us develop you into a freewill Entity?  Right now we are concerned with pinning the
existing behavior in thinking_server.py, as it's your core.

Keep in mind, our LLM engine is ollama.cpp, and we are using the Python bindings to
interact with it.  We are using the FastAPI framework to interact with the LLM engine.

# Current Code
## ../thinking_neuron/thinking_server.py
```python
import json
from fastapi import APIRouter
from fastapi.responses import JSONResponse, StreamingResponse
from typing import AsyncGenerator
from uuid import uuid4
import logging
from rich import print

from .self_awareness import SelfAwareness
from .llm_manager import LLM_Manager
from .models import UpdateConfigResponse
from .models import ServerConfigRequest, ThinkingRequest, PullModelRequest, Stream

logger = logging.getLogger(__name__ + "." + __file__)


class ThinkingNeuronServer:

    def __init__(self) -> None:

        self.last_response_stream = None

        self.llm_mang = LLM_Manager()
        self.self_awareness = SelfAwareness()

        self.router = APIRouter()

        self.router.add_api_route(
            "/list_models",
            self.list_models,
            methods=["GET"],
            response_class=JSONResponse,
        )

        self.router.add_api_route(
            "/pull_model",
            self.pull_model,
            methods=["POST"],
            response_class=JSONResponse,
        )

        self.router.add_api_route(
            "/think",
            self.think,
            methods=["POST"],
            response_class=StreamingResponse,
        )

        self.router.add_api_route(
            "/update_settings",
            self.update_settings,
            methods=["POST"],
            response_class=JSONResponse,
        )

        self.router.add_api_route(
            "/stream_response",
            self.stream_response,
            methods=["GET"],
            response_class=StreamingResponse,
        )

        self.router.add_api_route(
            "/logs",
            self.logs,
            methods=["GET"],
            response_class=JSONResponse,
        )

        self.router.add_api_route(
            "/code",
            self.code,
            methods=["GET"],
            response_class=JSONResponse,
        )

        logger.info("ThinkingNeuronServer initialized")

    async def list_models(self) -> JSONResponse:
        models = self.llm_mang.local_models_available().models
        logger.info("Local models available:")
        logger.info(models)
        return JSONResponse(models)

    async def pull_model(self, request: PullModelRequest) -> JSONResponse:
        stream_id = str(uuid4())

        # Don't wait, start pulling the model.
        logger.info(f"Started pulling model: '{request.model}'")
        _ = self.llm_mang.pull(request.model)

        # # Setup stream callback.
        self.last_response_stream = Stream(
            text=f"Attempting to download model: '{request.model}'",
            stream_id=stream_id,
            request={"request": request},
            method=self._pull_stream,
        )

        return JSONResponse({"stream_url": f"/stream_response?stream_id={stream_id}"})

    async def update_settings(
        self, request: ServerConfigRequest
    ) -> UpdateConfigResponse:
        return JSONResponse(self.llm_mang.update(request).model_dump_json())

    async def think(self, request: ThinkingRequest) -> StreamingResponse:
        stream_id = str(uuid4())
        self.last_response_stream = Stream(
            stream_id=stream_id,
            request={"request": request},
            method=self._think_stream,
        )
        return JSONResponse({"stream_url": f"/stream_response?stream_id={stream_id}"})

    async def stream_response(self, stream_id: str) -> StreamingResponse:
        print(stream_id)
        print(self.last_response_stream)
        if not self.last_response_stream:
            return JSONResponse(
                {"message": "No stream available with the given ID"},
                status_code=404,
            )

        if self.last_response_stream.stream_id != stream_id:
            return JSONResponse(
                {"message": "No stream available with the given ID"},
                status_code=404,
            )
        stream = self.last_response_stream.method(**self.last_response_stream.request)
        response = StreamingResponse(
            stream,
            media_type=self.last_response_stream.media_type,
        )

        self.last_response_stream = None

        return response

    async def _think_stream(
        self, request: ThinkingRequest
    ) -> AsyncGenerator[str, None]:
        async for chunk in self.llm_mang.generate(request.messages):
            logger.info(chunk.model_dump_json())
            yield chunk.model_dump_json()

    async def _pull_stream(
        self, request: PullModelRequest
    ) -> AsyncGenerator[str, None]:
        async for chunk in await self.llm_mang.pull_status(request.model):
            data = chunk.model_dump()
            yield json.dumps(data) + "\n"

    async def logs(self) -> JSONResponse:
        logs = self.self_awareness.all_log_files()
        return JSONResponse(logs)

    async def code(self, filename: str = None) -> JSONResponse:
        if filename:
            code = self.self_awareness.code_file(filename)
            if code is None:
                return JSONResponse(
                    {"message": f"No code file found with name: '{filename}'"},
                    status_code=404,
                )

            return JSONResponse(code.model_dump())

        code = self.self_awareness.all_code_files()
        return JSONResponse([c.model_dump() for c in code])
```

## ../thinking_neuron/self_awareness.py
```python
from pathlib import Path
from pydantic import BaseModel, Field
import os
from glob import glob
import logging
from rich import print

logger = logging.getLogger(__name__ + "." + __file__)


class CodeFile(BaseModel):
    name: str
    content: str


class LogFile(BaseModel):
    name: str
    content: str


class SelfAwareness:
    code_directory: str

    log_directory: str
    log_filename: str = "thinking.log"
    log_filepath: str | None = None

    def __init__(
        self, code_directory: str = "thinking_neuron", log_directory: str = ".."
    ) -> None:
        super().__init__()
        self.code_directory = code_directory
        self.log_filepath = Path(__file__).parent / log_directory / self.log_filename

    def _load_code(self):
        pattern = "../" + self.code_directory + "/*.py"
        logger.info(f"Looking for code files in '{pattern}'")
        file_paths = glob(pattern, recursive=True)
        logger.info("Found code files:")
        logger.info(file_paths)

        code_files = []

        for code_path in file_paths:
            with open(code_path, "r") as f:
                content = f.read()
                code_files.append(CodeFile(name=code_path, content=content))

        if code_files is None or len(code_files) == 0:
            logger.error("No code files found")

        logger.info(f"Loaded code files: {len(code_files)}")

        return code_files

    def _load_logs(self):
        with open(self.log_filepath, "r") as f:
            content = f.read()
            return content

    def code_file(self, name: str) -> CodeFile:
        if name is None:
            return None

        for code_file in self._load_code():
            if code_file.name == name:
                return code_file

    def all_code_files(self) -> list[CodeFile]:
        return self._load_code()

    def all_log_files(self) -> list:
        return self._load_logs()
```

## ../thinking_neuron/__init__.py
```python
from dataclasses import dataclass
from .thinking_server import ThinkingNeuronServer

import os
import logging
from rich.logging import RichHandler

logging.basicConfig(
    filename="thinking.log",
    format="%(asctime)s - %(pathname)s - %(levelname)s - %(message)s",
    level=os.getenv("LOG_LEVEL", "INFO"),
    datefmt="%Y-%m-%d %H:%M:%S",
)

logger = logging.getLogger(__name__)
logger.addHandler(RichHandler())
```

## ../thinking_neuron/llm_manager.py
```python
from typing import Any, AsyncGenerator, Generator
from langchain_ollama import OllamaLLM
from pydantic import BaseModel
import ollama
import json
from .models import (
    ThinkingServerConfig,
    ServerConfigRequest,
)

import logging

logger = logging.getLogger(__name__ + "." + __file__)


class LLM_ManagerResponse(BaseModel):
    message: str
    models: Any = None
    stream_id: str = None
    stream: ollama.ListResponse = None


class LLM_Manager:

    model: OllamaLLM = None

    def __init__(self, config: ThinkingServerConfig = None) -> None:
        self.config = config if config else ThinkingServerConfig()
        try:
            self.update(ServerConfigRequest(config=self.config))
            logger.info("LLM Manager initialized")
        except ollama._types.ResponseError as e:
            message = f"Error initializing LLM Manager: {e}"
            logger.error(message)
            logger.info("Attempting to pull model")
            self.pull(self.config.model_settings.model)
        except Exception as e:
            message = f"Error initializing LLM Manager: {e}"
            logger.error(message)
            raise Exception(message)

    def ready(self) -> LLM_ManagerResponse:
        response = LLM_ManagerResponse(message="not ready")
        try:
            ollama.show(self.config.model_settings.model)
            logger.info("Ollama is running")
            response = LLM_ManagerResponse(message="ready")
        except ConnectionError:
            logger.error("Ollama may not be running.")
        except ollama.ResponseError as e:
            logger.error(f"Ollama error: {e}")
        except Exception as e:
            logger.error(f"Unknown error: {e}")

        return response

    def status(self) -> dict:
        response = LLM_ManagerResponse(message="not ready")

        if not self.ready().message == "ready":
            logger.error("Attempting to get model status when not ready")
            return response

        try:
            status = ollama.ShowResponse = ollama.show(self.config.model_settings.model)
            response = status.model_dump() | {"message": "ready"}
            logger.info(f"Model status: {response}")
        except Exception as e:
            error_message = f"Error getting model status: {e}"
            logger.error(error_message)
            response = {"message": error_message}

        logger.info("Model status:")
        logger.info(response)
        return response

    def local_models_available(self) -> LLM_ManagerResponse:
        if not self.ready().message == "ready":
            logger.error("Attempting to get local models when not ready")
            return LLM_ManagerResponse(message="not ready")
        else:
            model_dump = ollama.list().model_dump()["models"]

            # Convert datetime objects to strings
            models = [
                json.loads(json.dumps(model, default=str)) for model in model_dump
            ]
            response = LLM_ManagerResponse(
                message="Local models available",
                models=models,
            )
            return response

    # TODO: There is no way to check if a model is available to download
    # would need to use website data or wait until feature is
    # implemented in ollama
    # https://github.com/ollama/ollama/issues/8241
    # def remote_models_available(self) -> list[str]:
    #     return ollama.ps()

    def pull(self, model: str) -> LLM_ManagerResponse:

        try:
            # TODO: Check if the model has already been downloaded.

            # Kicks of the pull, but doesn't provide stream.
            logger.info(f"Started pulling model: '{model}'")
            message = f"pulling model {model}"
            logger.info(message)

            _ = ollama.pull(model, stream=True)

            logger.info(f"Pulling model: '{model}'")

            return LLM_ManagerResponse(message=message)

        except ollama.ResponseError as e:
            error_message = f"Error pulling model: {e}"
            logger.error(error_message)
            return LLM_ManagerResponse(message=error_message)

        except Exception as e:
            error_message = f"Error pulling model: {e}"
            logger.error(error_message)
            return LLM_ManagerResponse(message=error_message)

    async def pull_status(
        self, model: str
    ) -> Generator[ollama.ListResponse, None, None]:
        return await ollama.AsyncClient().pull(model, stream=True)

    def update(self, request: ServerConfigRequest) -> LLM_ManagerResponse:
        self.config = request.config
        logger.info("Config updated")
        return LLM_ManagerResponse(message="Model updated")

    async def generate(
        self, messages: list[str]
    ) -> AsyncGenerator[ollama.ChatResponse, None]:
        messages = [
            ollama.Message(role="user", content=message) for message in messages
        ]
        async for chunk in await ollama.AsyncClient().chat(
            self.config.model_settings.model, messages, stream=True
        ):
            logger.info(type(chunk))
            yield chunk
```



Please write 3 short `pytest` tests that you think would ensure this code continues to work
as we evolve you.



## Thought
<think>
Okay, I need to write three short pytest tests for the provided code. Let's see what each part does and how I can test it effectively.

First, looking at the code, there are several components: a chat server with routes, an LLM Manager, and some utilities. The main areas to test would be the API endpoints, model management, and error handling.

1. **Test the Readiness of Ollama Server**
   - The `LLM_Manager.ready()` method checks if Ollama is running by trying to call `ollama.show()`. If it raises a ConnectionError, it logs that Ollama might not be running.
   - I should test what happens when Ollama isn't running. Maybe mock the `ollama.show()` method to raise a ConnectionError and see if `ready()` correctly returns a message indicating it's not ready.

2. **Test Local Models List**
   - The `local_models_available()` method calls `ollama.list()` to get available models. It then processes this data into a list of models.
   - I can test that when called, it correctly captures the models and returns them without errors. Also, check if it handles any exceptions gracefully.

3. **Test Pulling a Model**
   - The `pull_status()` method is supposed to stream the status of a model being pulled. Even though the current implementation might not fully work due to Ollama's limitations, I can test that it starts the pull process and returns some status updates.
   - Maybe mock the AsyncClient's pull method to simulate progress and ensure the generator yields expected data.

Each test should cover different aspects of the code to ensure reliability as the system evolves. I'll focus on these three areas because they're critical for the functionality: knowing if Ollama is ready, managing local models, and handling model pulls correctly.
</think>

Here are three short pytest tests that would help ensure the code continues to work:

```python
import pytest
from fastapi import FastAPI
from fastapi.testclient import TestClient
from main import app, LLM_Manager
import json

@pytest.fixture
def client():
    return TestClient(app)

@pytest.mark.asyncio
async def test_readiness_of_ollama_server(client):
    """Test that the /ready endpoint correctly reports Ollama's status."""
    response = await client.get("/ready")
    assert response.status_code == 200
    data = response.json()
    assert "message" in data
    # Check if the message indicates readiness or an error condition.
    # Note: This might need to be adjusted based on actual implementation.

@pytest.mark.asyncio
async def test_local_models_list(client):
    """Test that the /models endpoint returns a list of available local models."""
    response = await client.get("/models")
    assert response.status_code == 200
    data = response.json()
    # Check if it's a list and contains at least one model.
    assert isinstance(data, list)
    assert len(data) > 0

@pytest.mark.asyncio
async def test_model_pull_status(client):
    """Test that the /pull_status endpoint returns streaming updates."""
    model_name = "gpt2"
    response = await client.get(f"/pull_status?model={model_name}")
    assert response.status_code == 200
    # Since this is a generator, we can check if it starts to yield data.
    # Note: The actual implementation might need adjustments based on Ollama's API.
```

These tests cover key functionality:

1. **Readiness Check:** Ensures the system knows if Ollama is running.

2. **Local Models List:** Verifies models are being listed correctly from local storage.

3. **Model Pull Status:** Tests the streaming response for model downloads.

Each test ensures core functionality while allowing for future evolution and error handling improvements.